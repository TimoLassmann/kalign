name: Benchmark

on:
  push:
    branches: [main]
    paths:
      - 'lib/**'
      - 'python-kalign/**'
      - 'benchmarks/**'
      - 'CMakeLists.txt'
      - '.github/workflows/benchmark.yml'
  pull_request:
    branches: [main]
    paths:
      - 'lib/**'
      - 'python-kalign/**'
      - 'benchmarks/**'
      - 'CMakeLists.txt'
      - '.github/workflows/benchmark.yml'
  workflow_dispatch:

permissions:
  contents: write
  deployments: write

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libomp-dev cmake

      - name: Build C binary
        run: |
          mkdir -p build
          cd build
          cmake ..
          make -j$(nproc)

      - name: Install Python package
        run: |
          python -m pip install --upgrade pip
          python -m pip install -e .

      - name: Cache BAliBASE dataset
        uses: actions/cache@v4
        with:
          path: benchmarks/data/downloads
          key: benchmark-datasets-balibase-v1

      - name: Run benchmarks
        run: |
          python -m benchmarks \
            --dataset balibase \
            --method python_api cli \
            --binary build/src/kalign \
            --output benchmarks/results/latest.json \
            -v

      - name: Convert results for github-action-benchmark
        run: |
          python -c "
          import json
          with open('benchmarks/results/latest.json') as f:
              data = json.load(f)
          entries = []
          for method, stats in data.get('summary', {}).items():
              entries.append({
                  'name': f'SP Score Mean ({method})',
                  'unit': 'score',
                  'value': round(stats['sp_mean'], 2),
                  'range': f\"{stats['sp_min']:.1f}-{stats['sp_max']:.1f}\",
              })
              entries.append({
                  'name': f'Total Time ({method})',
                  'unit': 'seconds',
                  'value': round(stats['total_time'], 2),
              })
          with open('benchmarks/results/benchmark_output.json', 'w') as f:
              json.dump(entries, f, indent=2)
          "

      - name: Store benchmark result
        if: github.ref == 'refs/heads/main'
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'customBiggerIsBetter'
          output-file-path: benchmarks/results/benchmark_output.json
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: dev/bench
          auto-push: true
          alert-threshold: '95%'
          comment-on-alert: true
          fail-on-alert: false

      - name: Compare with baseline (PRs only)
        if: github.event_name == 'pull_request'
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'customBiggerIsBetter'
          output-file-path: benchmarks/results/benchmark_output.json
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: dev/bench
          auto-push: false
          alert-threshold: '95%'
          comment-on-alert: true
          fail-on-alert: true

      - name: Upload results artifact
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmarks/results/
